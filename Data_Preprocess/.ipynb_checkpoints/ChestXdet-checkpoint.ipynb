{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0c8325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "from Model.model import Mask_SAM\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddc9a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce397e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n\"file_name\": \"xxx.png\",\\n\\n\"syms\": [s1, s2, ...],\\n\\n\"boxes\": [[x1, y1, x2, y2], [x1, y1, x2, y2], …],\\n\\n\"polygons\": [[[x3, y3], [x4, y4], …], [[x3, y3], [x4, y4], …], ...]\\n\\n}\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "{\n",
    "\"file_name\": \"xxx.png\",\n",
    "\n",
    "\"syms\": [s1, s2, ...],\n",
    "\n",
    "\"boxes\": [[x1, y1, x2, y2], [x1, y1, x2, y2], …],\n",
    "\n",
    "\"polygons\": [[[x3, y3], [x4, y4], …], [[x3, y3], [x4, y4], …], ...]\n",
    "\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7862c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2class= { \"Effusion\":'0',\n",
    "               \"Nodule\":'1', \n",
    "               \"Cardiomegaly\":'2', \n",
    "               \"Fibrosis\":'3', \n",
    "               \"Consolidation\":'4', \n",
    "               \"Emphysema\":'5', \n",
    "               \"Mass\":'6', \n",
    "               \"Fracture\":'7', \n",
    "               \"Calcification\":'8', \n",
    "               \"Pleural Thickening\":'9', \n",
    "               \"Pneumothorax\":'10', \n",
    "               \"Atelectasis\":'11', \n",
    "               \"Diffuse Nodule\":'12'}\n",
    "wrongclass = {\n",
    "    'Atelectasis':'0',\n",
    "    'Calcification':'1', \n",
    "    'Cardiomegaly':'2', \n",
    "    'Consolidation':'3', \n",
    "    'Diffuse Nodule':'4', \n",
    "    'Effusion':'5', \n",
    "    'Emphysema':'6', \n",
    "    'Fibrosis':'7', \n",
    "    'Fracture':'8', \n",
    "    'Mass':'9', \n",
    "    'Nodule':'10', \n",
    "    'Pleural Thickening':'11', \n",
    "    'Pneumothorax':'12' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49451645",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/home/kappa7077/ChexDet_Data\"\n",
    "train_dir = os.path.join(root_path,'train')\n",
    "train_files = [os.path.join(train_dir,filename) for filename in os.listdir(train_dir)]\n",
    "test_dir = os.path.join(root_path,'test')\n",
    "test_files = [os.path.join(test_dir,filename) for filename in os.listdir(test_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d8447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(root_path,'train.json'), 'r') as f:\n",
    "    train_ann = json.load(f)\n",
    "with open(os.path.join(root_path,'test.json'), 'r') as f:\n",
    "    test_ann = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be28b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.new(\"RGB\", (1024, 1024), (0, 0, 0))\n",
    "image.save('/home/kappa7077/ChexDet_Data/NULL.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0840591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_polygon(polygons, image_size=(1024, 1024), fill_color=(0, 0, 0), outline_color=(255, 255, 255)):\n",
    "    image = Image.new(\"RGB\", image_size, fill_color)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for i, polygon in enumerate(polygons):\n",
    "        polygon = [(int(x), int(y)) for x, y in polygon]\n",
    "        draw.polygon(polygon, fill=outline_color, outline=outline_color)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9d50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Preprocess(datas,istrain,nullpath='/home/kappa7077/ChexDet_Data/NULL.png'):\n",
    "    Cache=[]\n",
    "    Train_Data=[]\n",
    "    BlankCount= [0] * len(label2class)\n",
    "    for data in tqdm(datas):\n",
    "        _dict = {}\n",
    "        Cache_dict={'0': '','1': '','2': '','3': '','4': '','5': '','6': '','7': '','8': '','9': '','10': '','11': '','12': '',}\n",
    "        img_name,classes,polygons = data['file_name'],data['syms'],data['polygons']\n",
    "        mask_dir = 'train_mask' if istrain else 'test_mask'\n",
    "        save_root = os.path.join(root_path,mask_dir,img_name)\n",
    "        img_path  = os.path.join(root_path,mask_dir.replace('_mask',''),img_name)\n",
    "        for cls, polygon in zip(classes, polygons):\n",
    "            if cls in _dict:\n",
    "                _dict[cls].append(polygon)\n",
    "            else:\n",
    "                Cache_dict[label2class[cls]] = save_root.replace('.png',f'_{cls}.png')\n",
    "                _dict[cls] = [polygon]\n",
    "#         for cls,polyhons in _dict.items(): \n",
    "#             mask = draw_polygon(polyhons)\n",
    "#             mask.save(Cache_dict[label2class[cls]])\n",
    "        num= img_name.replace('.png','')\n",
    "        for c,path in Cache_dict.items():\n",
    "            if path == \"\":\n",
    "                continue\n",
    "                idx = int(c)\n",
    "                Train_Data.append([img_path,c,nullpath,num])\n",
    "                BlankCount[idx]+=1\n",
    "            else:\n",
    "                Train_Data.append([img_path,c,path,num])\n",
    "        Cache.append([img_path,Cache_dict,'',num])\n",
    "    return Cache,Train_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6731a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [00:00<00:00, 155145.14it/s]\n",
      "100%|██████████| 553/553 [00:00<00:00, 174224.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3025 5165 553 1172 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Train_Cache,Train_Data=Preprocess(train_ann,True)\n",
    "Test_Cache,Test_Data=Preprocess(test_ann,False)\n",
    "val_data = random.sample(Test_Data, len(Test_Data) // 3)\n",
    "print(len(Train_Cache),len(Train_Data),len(Test_Cache),len(Test_Data),len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a51b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1185, 313, 223, 418, 1349, 137, 115, 313, 164, 412, 154, 249, 133]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Count= [0] * len(label2class)\n",
    "for data in Train_Data:\n",
    "    Count[int(data[1])]+=1\n",
    "print(Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e0fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, impath):\n",
    "    img = Image.open(impath)\n",
    "    img = np.array(img.convert('L'))\n",
    "    img = cv2.resize(img, (model.img_size, model.img_size), interpolation=cv2.INTER_CUBIC)\n",
    "    img = (img / 255.0).astype(np.float32)\n",
    "    img = torch.as_tensor(img)\n",
    "    if img.dim() == 2: # for gray image\n",
    "        img = img.unsqueeze(0)\n",
    "        img = repeat(img, 'c h w -> (repeat c) h w', repeat=3)\n",
    "    with torch.no_grad():\n",
    "        img_embeddings = model.sam.image_encoder(img.unsqueeze(0).to(model.device)).detach().cpu().numpy()\n",
    "    return img_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1f79c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'USE_TEXT_PROMPT': True, 'USE_MASK_PROMPT': True, 'USE_LORA': False}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "data_config_p= 'data_config.yml' \n",
    "model_config_p= 'model_config.yml' \n",
    "with open(data_config_p, 'r') as f:\n",
    "        data_config_p = yaml.load(f, Loader=yaml.FullLoader)\n",
    "with open(model_config_p, 'r') as f:\n",
    "    model_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_config['img_size'] = 512\n",
    "model = Mask_SAM(model_config, \"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e508073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GetFeatures...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [05:20<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComputeMostDifference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [50:18<00:00,  1.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CollectData...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/chen0063/Chexdet/Priority.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollectData...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39mDiff)\n\u001b[0;32m---> 33\u001b[0m np\u001b[38;5;241m.\u001b[39msavez(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/chen0063/Chexdet/Priority.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,indices\u001b[38;5;241m=\u001b[39msorted_indices)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/npyio.py:639\u001b[0m, in \u001b[0;36msavez\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_savez_dispatcher)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavez\u001b[39m(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    557\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save several arrays into a single file in uncompressed ``.npz`` format.\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m    Provide arrays as keyword arguments to store them under the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m \n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m     _savez(file, args, kwds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/npyio.py:736\u001b[0m, in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    734\u001b[0m     compression \u001b[38;5;241m=\u001b[39m zipfile\u001b[38;5;241m.\u001b[39mZIP_STORED\n\u001b[0;32m--> 736\u001b[0m zipf \u001b[38;5;241m=\u001b[39m zipfile_factory(file, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m namedict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    739\u001b[0m     fname \u001b[38;5;241m=\u001b[39m key \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/numpy/lib/npyio.py:103\u001b[0m, in \u001b[0;36mzipfile_factory\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[1;32m    102\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallowZip64\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MaskSam/lib/python3.11/zipfile.py:1286\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1288\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/chen0063/Chexdet/Priority.npz'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "features = []\n",
    "labels=[]\n",
    "def GetImageFeatures(Train_Cache,model):\n",
    "    for data in tqdm(Train_Cache):\n",
    "        impath = data[0]\n",
    "        label_num = []\n",
    "        for label,path in data[1].items():\n",
    "            if path != \"\":\n",
    "                label_num.append(int(label))\n",
    "        features.append(extract_features(model,impath)[0])\n",
    "        labels.append(label_num)\n",
    "    return np.array(features),labels\n",
    "def ComputeMostDifference(features):\n",
    "    sum_of_loss = []\n",
    "    for i in tqdm(range(features.shape[0])):\n",
    "        s = 0\n",
    "        for j in range(features.shape[0]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                s += np.mean((features[i] - features[j]) ** 2) \n",
    "        sum_of_loss.append(s)\n",
    "    return np.array(sum_of_loss)\n",
    "\n",
    "print(\"GetFeatures...\")\n",
    "Features,labels = GetImageFeatures(Train_Cache,model)\n",
    "print(\"ComputeMostDifference...\")\n",
    "Diff = ComputeMostDifference(Features)\n",
    "print(\"CollectData...\")\n",
    "sorted_indices = np.argsort(-Diff)\n",
    "np.savez(\"/work/chen0063/Chexdet/Priority.npz\",indices=sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ab7c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "these labels are enough!\n",
      "265\n"
     ]
    }
   ],
   "source": [
    "COUNT = np.zeros(len(label2class)+1, dtype=int)\n",
    "threshold = 16\n",
    "limit = len(label2class)*threshold\n",
    "# 遍歷排序好的索引，對標籤列表進行運算\n",
    "outputs=[]\n",
    "for idx in sorted_indices:\n",
    "    PastGreater = np.where(COUNT > threshold)[0]\n",
    "    if len(labels[idx]) == 0:\n",
    "        labels[idx].append(len(label2class))\n",
    "    set1 = set(labels[idx])\n",
    "    set2 = set(PastGreater)\n",
    "    if (set1.issubset(set2)) & (len(outputs) > limit):\n",
    "        print('these labels are enough!')\n",
    "        continue\n",
    "    COUNT[labels[idx]] += 1\n",
    "    Greater = np.where(COUNT > threshold)[0]\n",
    "    outputs.append(idx)\n",
    "    if len(Greater) == len(label2class)+1:\n",
    "        break\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "15c32d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"/work/chen0063/Chexdet/cache_list.npz\",cache=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e889d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/work/chen0063/Chexdet/train/36200.png',\n",
       " {'0': '',\n",
       "  '1': '',\n",
       "  '2': '',\n",
       "  '3': '',\n",
       "  '4': '',\n",
       "  '5': '',\n",
       "  '6': '',\n",
       "  '7': '',\n",
       "  '8': '',\n",
       "  '9': '',\n",
       "  '10': '',\n",
       "  '11': '',\n",
       "  '12': ''},\n",
       " '',\n",
       " '36200']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for gp in range(num_shots):\n",
    "    features_of_image = []                \n",
    "    for i in range(num_in_each_group):\n",
    "        image_index = num_in_each_group * gp + i\n",
    "        features_of_image.append(self.extract_features(model, items[image_index]))\n",
    "\n",
    "    features_of_image = np.array(features_of_image)\n",
    "\n",
    "    # brute force calculate the sum of loss\n",
    "    sum_of_loss = []\n",
    "    for i in range(num_in_each_group):\n",
    "        s = 0\n",
    "        for j in range(num_in_each_group):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                s += np.mean((features_of_image[i] - features_of_image[j]) ** 2)         \n",
    "        sum_of_loss.append(s)\n",
    "\n",
    "    selected_data_index = num_in_each_group * gp + np.argmax(np.array(sum_of_loss))\n",
    "    \n",
    "    output.append(items[selected_data_index])\n",
    "print('cache size: ', len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da06ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainD={}\n",
    "TrainD.update({'train':Train_Data})\n",
    "TrainD.update({'val':val_data})\n",
    "TrainD.update({'test':Test_Data})\n",
    "CacheD={}\n",
    "CacheD.update({'train':Train_Cache})\n",
    "CacheD.update({'val':[]})\n",
    "CacheD.update({'test':Test_Cache})\n",
    "with open('/home/kappa7077/ChexDet_Data/cache_wrong.json', 'w+') as f:\n",
    "    json.dump(CacheD, f)\n",
    "with open('/home/kappa7077/ChexDet_Data/data_AllBlank_wrong.json', 'w+') as f:\n",
    "    json.dump(TrainD, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60511944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice loss: 0.5004884600639343\n",
      "Dice loss per sample: 0.5004894137382507\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 原始版本的 Dice Loss 函数\n",
    "def dice_loss(predict, target):\n",
    "    assert predict.size() == target.size(), \"the size of predict and target must be equal.\"\n",
    "    epsilon = 1e-5\n",
    "    num = predict.size(0)\n",
    "    \n",
    "    pre = torch.sigmoid(predict).view(num, -1)\n",
    "    tar = target.view(num, -1)\n",
    "    \n",
    "    intersection = (pre * tar).sum(-1).sum()  \n",
    "    union = (pre + tar).sum(-1).sum()\n",
    "    \n",
    "    score = 1 -  (2.0 * intersection + epsilon) / (union + epsilon)\n",
    "    \n",
    "    return 'Dice loss', score\n",
    "\n",
    "# 修改后的 Dice Loss 函数，分别计算每个样本的 Dice Loss\n",
    "def dice_loss_per_sample(predict, target):\n",
    "    assert predict.size() == target.size(), \"the size of predict and target must be equal.\"\n",
    "    epsilon = 1e-5\n",
    "    num = predict.size(0)\n",
    "    \n",
    "    pre = torch.sigmoid(predict).view(num, -1)\n",
    "    tar = target.view(num, -1)\n",
    "    \n",
    "    intersection = (pre * tar).sum(-1)  \n",
    "    union = (pre + tar).sum(-1)\n",
    "    \n",
    "    score = 1 -  (2.0 * intersection + epsilon) / (union + epsilon)\n",
    "    \n",
    "    return 'Dice loss per sample', score\n",
    "\n",
    "# 随机生成数据\n",
    "predict = torch.randn(10, 3, 224, 224)\n",
    "target = torch.randn(10, 3, 224, 224) > 0\n",
    "\n",
    "# 测试原始版本的 Dice Loss 函数\n",
    "name, loss = dice_loss(predict, target)\n",
    "print(f\"{name}: {loss}\")\n",
    "\n",
    "# 测试修改后的 Dice Loss 函数\n",
    "name, losses = dice_loss_per_sample(predict, target)\n",
    "print(f\"{name}: {losses.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302403b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
